{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Amy Edwards\n",
    "#William Chirciu\n",
    "#Final Project\n",
    "#CSC 575 - Online 810\n",
    "#March 17,2019\n",
    "\n",
    "#Scikit Template based on Casey Bennett's from 2018\n",
    "#importing necessary packages for model evaluation\n",
    "import sys\n",
    "import csv\n",
    "import math\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "import time\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor , Ridge\n",
    "from sklearn.svm import SVR \n",
    "from sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, BaggingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_selection import RFE, VarianceThreshold, SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression, mutual_info_classif, chi2\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_validate, train_test_split\n",
    "from sklearn.preprocessing import KBinsDiscretizer, scale\n",
    "\n",
    "\n",
    "#Handle annoying warnings\n",
    "import warnings, sklearn.exceptions\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.ConvergenceWarning)\n",
    "\n",
    "\n",
    "#####################\n",
    "#\n",
    "# Global parameters\n",
    "#\n",
    "#####################\n",
    "\n",
    "target_idx=0           #Index of Target variable\n",
    "cross_val=1           #Control Switch for CV\n",
    "norm_target=0           #Normalize target switch\n",
    "norm_features=0              #Normalize target switch\n",
    "bin_cnt=2              #If bin target, this sets number of classes\n",
    "feat_select=0     #Control Switch for Feature Selection\n",
    "fs_type=3     #Feature Selection type (1=Stepwise Backwards Removal, 2=Wrapper Select, 3=Univariate Selection) \n",
    "feat_start=2  #Start column of features\n",
    "feat_start_pred = 1 # feature start for test set\n",
    "k_cnt='all'    #Number of 'Top k' best ranked features to select, only applies for fs_types 1 and 3\n",
    "\n",
    "## get id from test\n",
    "test_id = 0\n",
    "\n",
    "#Set global model parameters\n",
    "rand_st=0        #Set Random State variable for randomizing splits on runs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['target', 'id', 'cosine_title', 'cosine_description', 'cosine_attribute', 'query_ct', 'title_ct', 'desc_ct', 'jac_title', 'exact', 'title_query_ratio', 'desc_query_ratio', 'tq_common_ratio', 'dq_common_ratio']\n",
      "74067 74067\n",
      "\n",
      "\n",
      "['id', 'cosine_title', 'cosine_description', 'cosine_attribute', 'query_ct', 'title_ct', 'desc_ct', 'jac_title', 'exact', 'title_query_ratio', 'desc_query_ratio', 'tq_common_ratio', 'dq_common_ratio']\n",
      "112067\n",
      "112067\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######################\n",
    "#\n",
    "# Load Data\n",
    "#\n",
    "#####################\n",
    "\n",
    "file1= csv.reader(open('forRegression_jac.csv'), delimiter=',', quotechar='\"')\n",
    "file2= csv.reader(open('forRegressionTest_jac.csv'), delimiter=',', quotechar='\"')\n",
    "\n",
    "\n",
    "##########\n",
    "# train\n",
    "#########\n",
    "\n",
    "#Read Header Line\n",
    "header=next(file1)            \n",
    "\n",
    "#Read data\n",
    "data=[]\n",
    "target=[]\n",
    "for row in file1:\n",
    "    #Load Target\n",
    "    if row[target_idx]=='':                         #If target is blank, skip row                       \n",
    "        continue\n",
    "    else:\n",
    "        target.append(float(row[target_idx]))       #If pre-binned class, change float to int\n",
    "\n",
    "    #Load row into temp array, cast columns  \n",
    "    temp=[]\n",
    "                 \n",
    "    for j in range(feat_start,len(header)):\n",
    "        if row[j]=='':\n",
    "            temp.append(float())\n",
    "        else:\n",
    "            temp.append(float(row[j]))\n",
    "\n",
    "    #Load temp into Data array\n",
    "    data.append(temp)\n",
    "\n",
    "#Test Print\n",
    "print(header)\n",
    "print(len(target),len(data))\n",
    "print('\\n')\n",
    "\n",
    "data_np=np.asarray(data)\n",
    "target_np=np.asarray(target)\n",
    "\n",
    "\n",
    "###########\n",
    "# predict\n",
    "##########\n",
    " \n",
    "#Read Header Line\n",
    "header2=next(file2)            \n",
    "\n",
    "#Read data\n",
    "data2=[]\n",
    "test_id_list = []\n",
    "for row in file2:\n",
    "    #Load row into temp array, cast columns  \n",
    "    temp2=[]\n",
    "     \n",
    "    for i in range(test_id, (test_id + 1)):\n",
    "        if row[i]=='':\n",
    "            test_id_list.append(float())\n",
    "        else:\n",
    "            test_id_list.append(float(row[i]))\n",
    "            \n",
    "    for j in range(feat_start_pred,len(header2)):\n",
    "        if row[j]=='':\n",
    "            temp2.append(float())\n",
    "        else:\n",
    "            temp2.append(float(row[j]))\n",
    "\n",
    "    #Load temp into Data array\n",
    "    data2.append(temp2)\n",
    "\n",
    "#Test Print\n",
    "print(header2)\n",
    "print(len(data2))\n",
    "print(len(test_id_list))\n",
    "print('\\n')\n",
    "\n",
    "TestData = np.asarray(data2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing- normalizing and feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--LOW VARIANCE FILTER ON-- \n",
      "\n",
      "Selected ['query_ct', 'title_ct', 'desc_ct']\n",
      "Features (total, selected): 8 3\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "#\n",
    "# Preprocess data\n",
    "#\n",
    "###########################\n",
    "\n",
    "\n",
    "if norm_target==1:\n",
    "    #Target normalization for continuous values\n",
    "    target_np=scale(target_np)\n",
    "\n",
    "if norm_features==1:\n",
    "    #Feature normalization for continuous values\n",
    "    data_np=scale(data_np)\n",
    "\n",
    "\n",
    "#############################\n",
    "#\n",
    "# Feature Selection\n",
    "#\n",
    "#############################\n",
    "\n",
    "\n",
    "#Feature Selection\n",
    "if feat_select==1:\n",
    "    '''Three steps:\n",
    "       1) Run Feature Selection\n",
    "       2) Get lists of selected and non-selected features\n",
    "       3) Filter columns from original dataset\n",
    "       '''\n",
    "    \n",
    "    print('--FEATURE SELECTION ON--', '\\n')\n",
    "    \n",
    "    ##1) Run Feature Selection #######\n",
    "    if fs_type==1:\n",
    "        #stepwise backward\n",
    "        rgr = RandomForestRegressor(n_estimators=500, max_depth=None, min_samples_split=3, criterion='mse', random_state=None)\n",
    "        sel = RFE(rgr, n_features_to_select=k_cnt, step=.1)\n",
    "        print('Stepwise Recursive Backwards - Random Forest: ')\n",
    "            \n",
    "        fit_mod=sel.fit(data_np, target_np)\n",
    "        print(sel.ranking_)\n",
    "        sel_idx=fit_mod.get_support()      \n",
    "\n",
    "    if fs_type==2:\n",
    "        #Wrapper Select via model\n",
    "    \n",
    "        rgr =GradientBoostingRegressor(loss = 'ls', n_estimators = 200, learning_rate = 0.03, max_depth = 10, min_samples_split = 500, max_features = 'auto',random_state = rand_st,subsample = 0.8)\n",
    "        sel = SelectFromModel(rgr, prefit=False, threshold='mean', max_features=None)\n",
    "        print ('Wrapper Select - Random Forest: ')\n",
    "            \n",
    "        fit_mod=sel.fit(data_np, target_np)    \n",
    "        sel_idx=fit_mod.get_support()\n",
    "\n",
    "    if fs_type==3:       \n",
    "    ######Only work if the Target is continuous###########\n",
    "        #Univariate Feature Selection - Mutual Info Regression\n",
    "        sel=SelectKBest(mutual_info_regression, k=k_cnt)\n",
    "        fit_mod=sel.fit(data_np, target_np)\n",
    "        print ('Univariate Feature Selection - Mutual Info: ')\n",
    "        sel_idx=fit_mod.get_support()\n",
    "\n",
    "        #Print ranked variables out sorted\n",
    "        temp=[]\n",
    "        scores=fit_mod.scores_\n",
    "        for i in range(feat_start, len(header)):            \n",
    "            temp.append([header[i], float(scores[i-feat_start])])\n",
    "\n",
    "        print('Ranked Features')\n",
    "        temp_sort=sorted(temp, key=itemgetter(1), reverse=True)\n",
    "        for i in range(len(temp_sort)):\n",
    "            print(i, temp_sort[i][0], ':', temp_sort[i][1])\n",
    "        print('\\n')\n",
    "\n",
    "    ##2) Get lists of selected and non-selected features (names and indexes) #######\n",
    "    temp=[]\n",
    "    temp_idx=[]\n",
    "    temp_del=[]\n",
    "    for i in range(len(data_np[0])):\n",
    "        if sel_idx[i]==1:                                                           #Selected Features get added to temp header\n",
    "            temp.append(header[i+feat_start])\n",
    "            temp_idx.append(i)\n",
    "        else:                                                                       #Indexes of non-selected features get added to delete array\n",
    "            temp_del.append(i)\n",
    "    print('Selected', temp)\n",
    "    print('Features (total/selected):', len(data_np[0]), len(temp))\n",
    "    print('\\n')\n",
    "            \n",
    "                \n",
    "    ##3) Filter selected columns from original dataset #########\n",
    "    header = header[0:feat_start]\n",
    "    for field in temp:\n",
    "        header.append(field)\n",
    "    data_np = np.delete(data_np, temp_del, axis=1)                                 #Deletes non-selected features by index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## example of output from Univariate Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Univariate Feature Selection - Mutual Info: \n",
    "Ranked Features\n",
    "0 cosine_title : 0.043382674792738385\n",
    "1 cosine_description : 0.03819413882648881\n",
    "2 query_ct : 0.010914201602489904\n",
    "3 cosine_attribute : 0.0008147022271858262\n",
    "4 title_ct : 7.134835456046318e-05\n",
    "5 desc_ct : 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test model split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "#\n",
    "# Train SciKit Models\n",
    "#\n",
    "##########################################\n",
    "\n",
    "#Test/Train split - if needed\n",
    "data_train, data_test, target_train, target_test = train_test_split(data_np, target_np, test_size=0.35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training without CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next set of marked down code shows how we tested our model without cross validation. These were the models we used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Regressors####\n",
    "if cross_val==0:\n",
    "    #SciKit Decision Tree Regressor\n",
    "    rgr = DecisionTreeRegressor(criterion='friedman_mse', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, max_features=None, random_state=rand_st)\n",
    "    rgr.fit(data_train, target_train)\n",
    "\n",
    "    scores_RMSE = math.sqrt(metrics.mean_squared_error(target_test,rgr.predict(data_test)))\n",
    "    print('Decision Tree RMSE:', scores_RMSE)\n",
    "    scores_Expl_Var = metrics.explained_variance_score(target_test,rgr.predict(data_test))\n",
    "    print('Decision Tree Expl Var:', scores_Expl_Var)\n",
    "    \n",
    "    #SciKit Bagging Regressor\n",
    "    start_ts=time.time()\n",
    "    dgr = DecisionTreeRegressor(criterion='mse', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, max_features=None, random_state=rand_st) \n",
    "    bag = BaggingRegressor(dgr, max_samples = 0.6, random_state = rand_st)\n",
    "    bag.fit(data_train, target_train)\n",
    "\n",
    "    scores_RMSE = math.sqrt(metrics.mean_squared_error(target_test,bag.predict(data_test)))\n",
    "    print('Decision Bag Tree RMSE:', scores_RMSE)\n",
    "    scores_Expl_Var = metrics.explained_variance_score(target_test,bag.predict(data_test))\n",
    "    print('Decision Bag Tree Expl Var:', scores_Expl_Var)\n",
    "    \n",
    "    #SciKit Decision Tree Regressor\n",
    "    fgr = RandomForestRegressor(criterion='friedman_mse', max_depth=None, min_samples_split=2, min_samples_leaf=1, max_features=None, random_state=rand_st)\n",
    "    fgr.fit(data_train, target_train)\n",
    "\n",
    "    scores_RMSE = math.sqrt(metrics.mean_squared_error(target_test,rgr.predict(data_test)))\n",
    "    print('RF RMSE:', scores_RMSE)\n",
    "    scores_Expl_Var = metrics.explained_variance_score(target_test,rgr.predict(data_test))\n",
    "    print('RF Expl Var:', scores_Expl_Var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see the output from the models above. The RMSE is rather high for all of them, so we did not keep them when picking a best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out[]:\n",
    "Decision Tree RMSE: 0.690063227758232\n",
    "Decision Tree Expl Var: -0.6678101231732545\n",
    "\n",
    "Decision Bag Tree RMSE: 0.5353676542726318\n",
    "Decision Bag Tree Expl Var: -0.0036655880468252633\n",
    "\n",
    "RF RMSE: 0.690063227758232\n",
    "RF Expl Var: -0.6678101231732545"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ExtraTrees() is another method of feature importance that we used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = ExtraTreeRegressor()\n",
    "model.fit(data_train, target_train)\n",
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the output from ExtraTrees. You can see how many of the features were effecting the model similarily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['cosine_title', 'cosine_description', 'cosine_attribute', 'query_ct', 'title_ct', 'desc_ct']\n",
    "[0.24507603           0.24542375           0.03545412       0.04614564 0.16035267 0.26754779]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training with CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the other models we tried with cross validation. We did Random Forest, Decision Tree, Gradient Boosting, Ada Boosting Linear Regression, Decision Tree with Bagging and Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Cross-Val Regressors####\n",
    "   \n",
    "if cross_val==1:\n",
    "    #Setup Crossval regression scorers\n",
    "    scorers = {'Neg_MSE': 'neg_mean_squared_error', 'expl_var': 'explained_variance'} \n",
    "    \n",
    "    #SciKit Random Forest Regressor - Cross Val\n",
    "    start_ts=time.time()\n",
    "    model_Forest = RandomForestRegressor(n_estimators = 100, max_features = 0.33, max_depth = None, random_state = rand_st, min_samples_split = 3) \n",
    "    scores = cross_validate(model_Forest, data_np, y = target_np, scoring = scorers, cv = 5)\n",
    "    scores_RMSE = np.asarray([math.sqrt(-x) for x in scores['test_Neg_MSE']])                                       #Turns negative MSE scores into RMSE\n",
    "    scores_Expl_Var = scores['test_expl_var']\n",
    "    print(\"Random Forest RMSE:: %0.2f (+/- %0.2f)\" % ((scores_RMSE.mean()), (scores_RMSE.std() * 2)))\n",
    "    print(\"Random Forest Expl Var: %0.2f (+/- %0.2f)\" % ((scores_Expl_Var.mean()), (scores_Expl_Var.std() * 2)))\n",
    "    print(\"CV Runtime:\", time.time()-start_ts)\n",
    "    \n",
    "    #SciKit Decision Tree Regressor - Cross Val\n",
    "    start_ts=time.time()\n",
    "    model_Decision = DecisionTreeRegressor(criterion='mse', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, max_features=None, random_state=rand_st)\n",
    "    scores = cross_validate(model_Decision, data_np, target_np, scoring=scorers, cv=5)\n",
    "    scores_RMSE = np.asarray([math.sqrt(-x) for x in scores ['test_Neg_MSE']]) #Turns negative MSE scores into RMSE\n",
    "    scores_Expl_Var = scores['test_expl_var']\n",
    "    print(\"Decision Tree RMSE:: %0.2f (+/- %0.2f)\" % ((scores_RMSE.mean()), (scores_RMSE.std() * 2)))\n",
    "    print(\"Decision Tree Expl Var: %0.2f (+/- %0.2f)\" % ((scores_Expl_Var.mean()), (scores_Expl_Var.std() * 2)))\n",
    "    print(\"CV Runtime:\", time.time()-start_ts)\n",
    "    \n",
    "    #SciKit Bagging Regressor\n",
    "    start_ts=time.time()\n",
    "    dgr = DecisionTreeRegressor(criterion='mse', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, max_features=None, random_state=rand_st) \n",
    "    bag = BaggingRegressor(dgr, max_samples = 0.6, random_state = rand_st)\n",
    "    scores = cross_validate(bag, data_np, target_np, scoring=scorers, cv=5)\n",
    "\n",
    "    scores_RMSE = np.asarray([math.sqrt(-x) for x in scores ['test_Neg_MSE']]) #Turns negative MSE scores into RMSE\n",
    "    scores_Expl_Var = scores['test_expl_var']\n",
    "    print(\"Decision Bag Tree RMSE:: %0.2f (+/- %0.2f)\" % ((scores_RMSE.mean()), (scores_RMSE.std() * 2)))\n",
    "    print(\"Decision Bag Tree Expl Var: %0.2f (+/- %0.2f)\" % ((scores_Expl_Var.mean()), (scores_Expl_Var.std() * 2)))\n",
    "    print(\"CV Runtime:\", time.time()-start_ts)\n",
    "    \n",
    "    #SciKit Ada Boosting - Cross Val\n",
    "    start_ts=time.time()\n",
    "    model_Ada=AdaBoostRegressor(loss = 'linear', n_estimators = 100, learning_rate = 0.5, base_estimator = None, random_state = rand_st)\n",
    "    scores = cross_validate(model_Ada, data_np, y = target_np, scoring = scorers, cv = 5)\n",
    "    scores_RMSE = np.asarray([math.sqrt(-x) for x in scores['test_Neg_MSE']])   #Turns negative MSE scores into RMSE                                    \n",
    "    scores_Expl_Var = scores['test_expl_var']\n",
    "    print(\"Ada Boosting RMSE: %0.2f (+/- %0.2f)\" % ((scores_RMSE.mean()), (scores_RMSE.std()* 2)))\n",
    "    print(\"Ada Boosting Expl Var: %0.2f (+/- %0.2f)\" % ((scores_Expl_Var.mean()), (scores_Expl_Var.std() * 2)))\n",
    "    print(\"CV Runtime:\", time.time()-start_ts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#SciKit Decision Tree Regressor - Cross Val\n",
    "if cross_val==1:\n",
    "    scorers = {'Neg_MSE': 'neg_mean_squared_error', 'expl_var': 'explained_variance'} \n",
    "    \n",
    "    start_ts=time.time()\n",
    "    model_Linear = LinearRegression()\n",
    "    scores = cross_validate(model_Linear, data_np, target_np, scoring=scorers, cv=5)\n",
    "    scores_RMSE = np.asarray([math.sqrt(-x) for x in scores ['test_Neg_MSE']]) #Turns negative MSE scores into RMSE\n",
    "    scores_Expl_Var = scores['test_expl_var']\n",
    "    print(\"Linear RMSE:: %0.2f (+/- %0.2f)\" % ((scores_RMSE.mean()), (scores_RMSE.std() * 2)))\n",
    "    print(\"Linear Expl Var: %0.2f (+/- %0.2f)\" % ((scores_Expl_Var.mean()), (scores_Expl_Var.std() * 2)))\n",
    "    print(\"CV Runtime:\", time.time()-start_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the outputs for the various models for this instance of running. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear RMSE:: 0.51 (+/- 0.02)\n",
    "Linear Expl Var: 0.10 (+/- 0.04)\n",
    "CV Runtime: 0.05126190185546875"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out[]:\n",
    "Random Forest RMSE:: 0.52 (+/- 0.02)\n",
    "Random Forest Expl Var: 0.07 (+/- 0.05)\n",
    "\n",
    "Decision Tree RMSE:: 0.70 (+/- 0.02)\n",
    "Decision Tree Expl Var: -0.76 (+/- 0.11)\n",
    "CV Runtime: 1.98085618019104\n",
    "\n",
    "Gradient Boosting RMSE: 0.50 (+/- 0.02)\n",
    "Gradient Boosting Expl Var: 0.11 (+/- 0.05)\n",
    "CV Runtime: 13.127336740493774\n",
    "\n",
    "Ada Boosting RMSE: 0.51 (+/- 0.01)\n",
    "Ada Boosting Expl Var: 0.10 (+/- 0.03)\n",
    "CV Runtime: 8.59136414527893"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    #SciKit Neural Network - Cross Val\n",
    "    start_ts=time.time()\n",
    "    model_NN=MLPRegressor(activation='logistic', solver = 'lbfgs', max_iter= 1000, hidden_layer_sizes = (10,), alpha = 0.0001, random_state = rand_st)\n",
    "    scores = cross_validate(model_NN, data_np, y = target_np, scoring = scorers, cv = 5)\n",
    "\n",
    "    scores_RMSE = np.asarray([math.sqrt(-x) for x in scores['test_Neg_MSE']])\n",
    "                 #Turns negative MSE scores into RMSE                                             \n",
    "    scores_Expl_Var = scores['test_expl_var']\n",
    "    print(\"Neural Net RMSE: %0.2f (+/- %0.2f)\" % ((scores_RMSE.mean()), (scores_RMSE.std()* 2)))\n",
    "    print(\"Neural Net Expl Var: %0.2f (+/- %0.2f)\" % ((scores_Expl_Var.mean()), (scores_Expl_Var.std() * 2)))\n",
    "    print(\"CV Runtime:\", time.time()-start_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out[]:\n",
    "Neural Net RMSE: 0.51 (+/- 0.02)\n",
    "Neural Net Expl Var: 0.10 (+/- 0.05)\n",
    "CV Runtime: 21.232121229171753"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#SciKit SVM - Cross Val\n",
    "    start_ts=time.time()\n",
    "    model_SVR =SVR(kernel = \"linear\", C = 1.0, gamma = 0.1)\n",
    "    scores=cross_validate(model_SVR, data_np, y = target_np, scoring = scorers, cv = 5)                                                                                                 \n",
    "    scores_RMSE = np.asarray([math.sqrt(-x) for x in scores['test_Neg_MSE']])                                       #Turns negative MSE scores into RMSE\n",
    "    scores_Expl_Var = scores['test_expl_var']\n",
    "    print(scores_RMSE)\n",
    "    print(\"SVM RMSE: %0.2f (+/- %0.2f)\" % ((scores_RMSE.mean()), (scores_RMSE.std() * 2)))\n",
    "    print(\"SVM Expl Var: %0.2f (+/- %0.2f)\" % ((scores_Expl_Var.mean()), (scores_Expl_Var.std() * 2)))\n",
    "    print(\"CV Runtime:\", time.time()-start_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out[]: (past data, took too long on current data)\n",
    "[0.51416566 0.50801402 0.50588603 0.51958252 0.55598427]\n",
    "SVM RMSE: 0.52 (+/- 0.04)\n",
    "SVM Expl Var: 0.06 (+/- 0.04)\n",
    "CV Runtime: 631.560005903244   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting RMSE: 0.4821 (+/- 0.0197)\n",
      "Gradient Boosting Expl Var: 0.1806 (+/- 0.0522)\n",
      "CV Runtime: 448.75373458862305\n"
     ]
    }
   ],
   "source": [
    "if cross_val==1:\n",
    "    #Setup Crossval regression scorers\n",
    "    scorers = {'Neg_MSE': 'neg_mean_squared_error', 'expl_var': 'explained_variance'} \n",
    "    #SciKit Gradient Boosting - Cross Val\n",
    "    start_ts=time.time()\n",
    "    #model= GradientBoostingRegressor(loss = 'ls', n_estimators = 200, learning_rate = 0.03, max_depth = 10, min_samples_split = 500, max_features = 'auto',random_state = rand_st,subsample = 0.8)\n",
    "    #model = Ridge(alpha=1, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver='auto', random_state=rand_st)\n",
    "    model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,random_state=rand_st, loss='ls')\n",
    "    scores = cross_validate(model, data_np, y = target_np, scoring = scorers, cv = 5)\n",
    "    scores_RMSE = np.asarray([math.sqrt(-x) for x in scores['test_Neg_MSE']])                                       #Turns negative MSE scores into RMSE\n",
    "    scores_Expl_Var = scores['test_expl_var']\n",
    "    print(\"Gradient Boosting RMSE: %0.4f (+/- %0.4f)\" % ((scores_RMSE.mean()), (scores_RMSE.std() * 2)))\n",
    "    print(\"Gradient Boosting Expl Var: %0.4f (+/- %0.4f)\" % ((scores_Expl_Var.mean()), (scores_Expl_Var.std() * 2)))\n",
    "    print(\"CV Runtime:\", time.time()-start_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to use the model to make predictions on the given test data and save it to a csv file for ranking in Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit final GB model\n",
    "model.fit(data_train, target_train)\n",
    "# make a prediction\n",
    "y_new = model.predict(TestData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the inputs and predicted outputs\n",
    "with open('test_predict.csv', 'w') as f:\n",
    "    f.write(\"id,relevance\\n\")\n",
    "    for i in range(len(TestData)):\n",
    "        #print(\"X=%s, Predicted=%s\" % (int(test_id_list[i]), y_new[i]))\n",
    "        f.write(\"%d, %.3f\\n\" % (int(test_id_list[i]), y_new[i]))\n",
    "        \n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
